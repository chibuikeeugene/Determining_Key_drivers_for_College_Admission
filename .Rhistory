#retrieving the structure of the dataframe and changing datatypes
str(collegedata)
a <- as.factor(admit_model, levels = 0:1)
a <- as.factor(admit_model)
str(admit_model)
confusionMatrix(table(factor(predict(admit_model), levels = 0:1), factor(admit_train$admit, levels = 0:1)))
#using the decision tree algorithm
library(rpart) #load the library rpart for tree classifier
admit_tree_model <- rpart(formula = admit ~ ., data = admit_train, method = 'class')
admit_tree_model
summary(admit_tree_model)
plot(admit_tree_model)
print(admit_tree_model)
printcp(admit_tree_model)
confusionMatrix(predict(admit_tree_model), admit_train$admit))
confusionMatrix(predict(admit_tree_model), admit_train$admit)
confusionMatrix(factor(predict(admit_tree_model), levels = 0:1), factor(admit_train$admit, levels = 0:1))
library(e1071)
svm_admit <- svm(admit ~ ., admit_train) #creating the train set model to predict the factors affecting 'churn' of mobile subscribers and assigning it to the variable svm_churn
summary(svm_admit)
confusionMatrix(admit_train$admit, predict(svm_admit), positive = '1')
#validation check using k-fold cross validation
?trainControl
a <- train(admit ~ ., data = new_normalized_collegedata, method = 'glm', trControl = trainControl(method = 'cv', number = 10, verboseIter = TRUE ))
print(a)
print(admit_model)
summary(admit_model)
print(a)
confusionMatrix(admit_train$admit, predict(svm_admit), positive = '1')
#validation check using k-fold cross validation
?train
getModelInfo()
View(new_normalized_collegedata)
summary(collegedata)
View(collegedata)
printcp(admit_tree_model)
plotcp(admit_tree_model)
#normalizing the gre and gpa data by obtaining its logarithmic values
str(collegedata)
d <- normalize(collegedata, method = "standardize", range = c(0, 1), margin = 1L, on.constant = "quiet")
View(d)
install.packages('normalr')
library(normalr)
d <- normalise(collegedata, method = "standardize", range = c(0, 1), margin = 1L, on.constant = "quiet")
View(d)
d <- normaliseData(collegedata, getLambda(collegedata, parallel = FALSE))
View(d)
#normalizing the gre and gpa data by obtaining its logarithmic values
str(collegedata)
?normaliseData
install.packages('BBmisc')
library(BBmisc)
d <- normalize(collegedata, method = "standardize", range = c(0, 1), margin = 1L,
on.constant = "quiet")
View(d)
a <- train(admit ~ ., data = d, method = 'glm', trControl = trainControl(method = 'cv', number = 10, verboseIter = TRUE ))
print(a)
normalized_data <- normalize(collegedata, method = "standardize", range = c(0, 1), margin = 1L,
on.constant = "quiet")
View(normalized_data)
pca_nncd <- PCA(normalized_data[c(2:3)], scale = TRUE)
View(normalized_data)
summary(pca_nncd)
sample_split <- floor(.7 * nrow(normalized_data)) #splitting the dataset by 70:30 ratio, i.e., extracting 70% of the data for training...
set.seed(1) # ensuring randomized variabeles are fixed. In order words we want to obtain the same variable results anytime we run our model
training_data <- sample(seq_len(nrow(normalized_data)), size = sample_split) # creating the randomized sample training dataset of which it is made uo of 70% of the entire dataset
admit_train <- normalized_data[training_data, ] #inserting the taining_data variable as an argument in the overall dataset(Customer_data)
admit_test <- normalized_data[-training_data, ]
admit_model <- glm(formula = admit ~ ., data = admit_train, family = binomial(link = 'logit'))
summary(admit_model)
print(admit_model)
a <- train(admit ~ ., data = normalized_data, method = 'glm', trControl = trainControl(method = 'cv', number = 10, verboseIter = TRUE ))
print(a)
admit_tree_model <- rpart(formula = admit ~ ., data = admit_train, method = 'class')
admit_tree_model
summary(admit_tree_model)
plotcp(admit_tree_model)
printcp(admit_tree_model)
tree_validation <- train(admit ~ ., data = normalized_data, method = 'class', trControl = trainControl(method = 'cv',number = 10, verboseIter = TRUE))
print(tree_validation)
svm_admit <- svm(admit ~ ., admit_train) #creating the train set model to predict the factors affecting 'churn' of mobile subscribers and assigning it to the variable svm_churn
summary(svm_admit)
confusionMatrix(admit_train$admit, predict(svm_admit), positive = '1')
plot(admit_tree_model)
#to enhance the decision tree plot we use rattle
install.packages('rattle')
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(admit_tree_model)
#to prune our tree for better optimization
ptree <- prune(admit_tree_model, + cp = admit_tree_model$cptable[which.min(fit$cptable[,'xerror']),'CP'])
?prune()
#to prune our tree for better optimization
ptree <- prune(admit_tree_model, cp = admit_tree_model$cptable[which.min(fit$cptable[,'xerror']),'CP'])
#to prune our tree for better optimization
ptree <- prune(admit_tree_model, cp = fit$cptable[which.min(fit$cptable[,'xerror']),'CP'])
#to validate the tree model we use the printcp
printcp(admit_tree_model)
admit_test$pred <- predict(object = admit_tree_model, newdata = admit_test,   type = "class")
confusionMatrix(data = admit_test$pred,  reference = normalized_data$admit)
confusionMatrix(data = factor(admit_test$pred, levels = 0:1),  reference = normalized_data$admit)
confusionMatrix(data = factor(admit_test$pred, levels = 0:1),  reference = factor(normalized_data$admit, levels = 0:1))
#assigning categories to the gpa scores
View(collegedata)
collegedata['gpa'].max
collegedata['gpa'].max()
summarise(collegedata['gpa'], max)
summarise(collegedata['gpa'], max())
?max.col()
?max
?max()
max(collegedata$gpa, na.rm = TRUE)
min(collegedata$gpa, na.rm = TRUE)
?apply()
group_gpa <-
for (i in collegedata$gpa)
{
if (i > 2.23 & i <= 2.99)
{
print('Low')
}
else if (i > 2.99 & i <= 3.5)
{
print('Medium')
}
else
print('High')
}
gpa_categories <- apply(collegedata$gpa, 1, group_gpa)
View(gpa_categories)
?cbind()
gpa_categories <-
for (i in collegedata$gpa)
{
if (i > 2.23 & i <= 2.99)
{
print('Low')
}
else if (i > 2.99 & i <= 3.5)
{
print('Medium')
}
else
print('High')
}
?cbind()
collegedata <- cbind(collegedata, gpa_categories)
View(collegedata)
collegedata <- rbind(collegedata, gpa_categories)
View(collegedata)
collegedata['gpa_categories'] <- rbind(collegedata, gpa_categories)
View(collegedata)
?transform
collegedata['gpa_categories'] <- gpa_categories
View(collegedata)
collegedata['gpa_categories'] <- gpa_categories
View(collegedata)
New_collegedata <- transform(collegedata, gpa_categories)
View(New_collegedata)
a <- gpa_categories
View(a)
View(gpa_categories)
gpa_categories <-
for (i in collegedata$gpa)
{
if (i > 2.23 & i <= 2.99)
{
print('Low')
}
else if (i > 2.99 & i <= 3.5)
{
print('Medium')
}
else
print('High')
}
View(gpa_categories)
collegedata['gpa_categories'] <-
for (i in collegedata$gpa)
{
if (i > 2.23 & i <= 2.99)
{
print('Low')
}
else if (i > 2.99 & i <= 3.5)
{
print('Medium')
}
else
print('High')
}
View(collegedata)
#Analyze the historical data and determine the key drivers for admission.
getwd()
library(ggplot2)
library(dplyr)
library(plyr)
collegedata <- read.csv('College_admission.csv')
View(collegedata)
summary(collegedata)
#finding missing data
apply(is.na(collegedata),2,sum)
par(mfrow = c(1,2))
boxplot(collegedata$gpa, main = 'Grade Point Average', col = 'orange',sub=paste("Outlier rows: ", boxplot.stats(collegedata$gpa)$out))
boxplot(collegedata$gre, main = 'Graduate Record Exam Scores', col = 'green', sub=paste("Outlier rows: ", boxplot.stats(collegedata$gre)$out))
#Detecting and handling outliers
#outliers can be detected via the use of box plot if they exist in our datframe
par(mfrow = c(1,2))
boxplot(collegedata$gpa, main = 'Grade Point Average', col = 'orange',sub=paste("Outlier rows: ", boxplot.stats(collegedata$gpa)$out))
boxplot(collegedata$gre, main = 'Graduate Record Exam Scores', col = 'green', sub=paste("Outlier rows: ", boxplot.stats(collegedata$gre)$out))
#To perform outlier treatment, we simply assign values closer to the median of each variable to the outlier fields.
#replacing the outlier in GPA
collegedata$gpa[collegedata['gpa'] == 2.26] <- 3.39
#checking to see if the outlier in index 290 in GPA has being replaced
collegedata$gpa[290]
#replacing the outlier in GRE
collegedata$gre[collegedata['gre'] == c(220, 300)] <- 560
#checking to see if the outlier values (220 and 300) has being replaced
collegedata$gre[collegedata['gre'] == c(220, 300)]
#retrieving the structure of the dataframe and changing datatypes
str(collegedata)
#changing certain numeric or integer data type to factor
collegedata$admit <- sapply(collegedata$admit, factor)
collegedata$ses <- sapply(collegedata$ses, factor)
collegedata$Gender_Male <- sapply(collegedata$Gender_Male, factor)
collegedata$Race <- sapply(collegedata$Race, factor)
collegedata$rank <- sapply(collegedata$rank, factor)
str(collegedata) # checking if the datatype changed accordingly
#checking if the data is normally distributed
#one way to check for normality is to examine if the mean, median and mode are the same.
summary(collegedata)
#another way to check for normality is to plot the normal distribution curve
library(e1071)
par(mfrow=c(1, 2))  # divide graph area in 2 columns
plot(density(collegedata$gpa), main="Density Plot: GPA", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(collegedata$gpa), 2)))  # density plot for 'GPA'
polygon(density(collegedata$gpa), col="orange")
plot(density(collegedata$gre), main="Density Plot: GRE", ylab="Frequency", sub=paste("Skewness:", round(e1071::skewness(collegedata$gre), 2)))  # density plot for 'GRE'
polygon(density(collegedata$gre), col="green")
#normalizing the dataset using the normailze function from the BBmisc package
install.packages('BBmisc')
library(BBmisc)
normalized_data <- normalize(collegedata, method = "standardize", range = c(0, 1), margin = 1L, on.constant = "quiet")
View(normalized_data)
library(FactoMineR)
pca_nncd <- PCA(normalized_data[c(2:3)], scale = TRUE)
summary(pca_nncd)
library(blorr)
sample_split <- floor(.7 * nrow(normalized_data)) #splitting the dataset by 70:30 ratio, i.e., extracting 70% of the data for training...
set.seed(1) # ensuring randomized variabeles are fixed. In order words we want to obtain the same variable results anytime we run our model
training_data <- sample(seq_len(nrow(normalized_data)), size = sample_split) # creating the randomized sample training dataset of which it is made uo of 70% of the entire dataset
admit_train <- normalized_data[training_data, ] #inserting the taining_data variable as an argument in the overall dataset(Customer_data)
admit_test <- normalized_data[-training_data, ] #here we are assigning the rest of the test dataset to the churn test variable as can be seen using '-' symbol in front of the training_data
#using the glm method
admit_model <- glm(formula = admit ~ ., data = admit_train, family = binomial(link = 'logit'))
summary(admit_model)
print(admit_model)
#accessing the accuracy of our logistics model and running validation checks with Confusion matrix
library(caret)
#validation check using k-fold cross validation
a <- train(admit ~ ., data = normalized_data, method = 'glm', trControl = trainControl(method = 'cv', number = 10, verboseIter = TRUE ))
print(a)
using the decision tree algorithm
library(rpart) #load the library rpart for tree classifier
#create a tree_model
admit_tree_model <- rpart(formula = admit ~ ., data = admit_train, method = 'class')
admit_tree_model
summary(admit_tree_model)
plotcp(admit_tree_model)
plot(admit_tree_model)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
fancyRpartPlot(admit_tree_model)
#to validate the tree model we use the printcp
printcp(admit_tree_model)
#create an SVM model
library(e1071)
svm_admit <- svm(admit ~ ., admit_train) #creating the train set model to predict the factors affecting 'churn' of mobile subscribers and assigning it to the variable svm_churn
summary(svm_admit)
confusionMatrix(admit_train$admit, predict(svm_admit), positive = '1')
max(collegedata$gpa)
min(collegedata$gpa)
New_collegedata <- transform(collegedata, gpa_categories = ifelse(gpa <= 2.99, 'Low', ifelse(gpa <= 3.49,'Medium', 'High' )))
View(New_collegedata)
View(gpa)
sum(New_collegedata$gpa)
a <- New_collegedata$gpa/sum(New_collegedata$gpa) * 100
View(a)
a <- New_collegedata$gpa/max(New_collegedata$gpa) * 100
View(a)
?round()
a <- round(New_collegedata$gpa/max(New_collegedata$gpa) * 100)'%'
View(a)
a <- round(New_collegedata$gpa/max(New_collegedata$gpa) * 100) + '%'
View(a)
a <- round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 1)
View(a)
a <- round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 0)
View(a)
a <- paste(round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 0),'%')
View(a)
a <- paste(round(New_collegedata$gpa/New_collegedata$gre) * 100, 0),'%')
View(a)
a <- New_collegedata$gpa/New_collegedata$gre * 100
View(a)
a <- round(New_collegedata$gpa/New_collegedata$gre * 100,4)
View(a)
max(New_collegedata$gre)
a <- paste(round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 0),'%')
View(a)
New_collegedata1 <- transform(New_collegedata, Probabilty_of_being_admitted = paste(round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 0),'%') )
View(New_collegedata1)
?plot()
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted')
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted')
?xlim
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', xlim = c(0,100))
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', ylim = c(0,100))
New_collegedata1 <- transform(New_collegedata, Probabilty_of_being_admitted = round(New_collegedata$gpa/max(New_collegedata$gpa) * 100, 2))
View(New_collegedata1)
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', ylim = c(0,100))
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', ylim = c(50,100))
New_collegedata1 <- transform(New_collegedata, Probabilty_of_being_admitted = round(New_collegedata$gpa/max(New_collegedata$gpa), 2))
View(New_collegedata1)
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', ylim = c(50,100))
plot(New_collegedata1$gpa, New_collegedata1$Probabilty_of_being_admitted, main = 'Graph of Grade point against Admission ratio', type = 'p', xlab = 'Grade Point Average', ylab = '% probability of being admitted', ylim = c(.5,1))
pca_nncd <- PCA(normalized_data[c(2:3)], scale = TRUE)
